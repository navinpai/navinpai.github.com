<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title> on Gradient Ascent </title>
    <link>http://navinpai.github.io/ga/</link>
    <language>en-us</language>
    <author>Alexander Ivanov</author>
    <updated>2016-10-14 00:00:00 &#43;0000 UTC</updated>
    
    <item>
      <title>Deconvolution Layer == Convolution Layer?</title>
      <link>http://navinpai.github.io/ga/post/deconv-layer-conv-layer/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>http://navinpai.github.io/ga/post/deconv-layer-conv-layer/</guid>
      <description>&lt;p&gt;I stumbled across &lt;a href=&#34;1609.07009&#34;&gt;a note&lt;/a&gt; which expands an already brilliant paper: &lt;a href=&#34;1609.05158&#34;&gt;Realtime Image/Video Super Resolution using a Sub-Pixel CNN&lt;/a&gt; (&lt;em&gt;which I really should write about soon&lt;/em&gt;). The note basically tries to answer the question: Is the decovolution layer the same as a convolution layer in Low Resolution (LR) space?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR: Yes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Their reasoning was what really caught my eye. The crux of it is:&lt;/p&gt;











  





  


&lt;blockquote&gt;
  &lt;p&gt;
We notice that the different sets of weights in the kernel (1,1,4,4) are activated independently from each other.So we can easily break them into kernels as shown in the figure on the right.This operation is invertible (4,1,2,2) because each set of the weights are independent from each other during the convolution. In our paper,instead of convolving the kernel with the unpooleded sub-pixel image, we notice that the different sets of weights in the kernel (1,1,4,4) are activated independently from each other. So we can easily break them into kernels as shown in the figure on the right.This operation is invertible (4,1,2,2) because each set of the weights are independent from each other during the convolution.In our paper,instead of convolving the kernel with the unpooled sub­pixel image, we convolve (1,1,4,4) the kernel with the LR input directly as illustrated by the following figure.
&lt;/p&gt;
    &lt;strong&gt;&lt;/strong&gt;
    
      
    
&lt;/blockquote&gt;

&lt;p&gt;Another interesting part of the note was the realization that the representation power of a network in LR is mucn more than that in HR (&lt;em&gt;where you first upscale using, say, Bicubic and then pass this as input to the network&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Go on, read the note (and the paper it expands) and the fascinating breakdown of the logic.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.07009&#34;&gt;Is the deconvolution layer the same as a convolutional layer?&lt;/a&gt;: &lt;em&gt;Wenzhe Shi, Jose Caballero, Lucas Theis, Ferenc Huszar, Andrew Aitken,Alykhan Tejani, Johannes Totz, Christian Ledig, Zehan Wang&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.05158&#34;&gt;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network &lt;/a&gt;: &lt;em&gt;Wenzhe Shi, Jose Caballero, Lucas Theis, Ferenc Huszar, Andrew Aitken,Alykhan Tejani, Johannes Totz, Christian Ledig, Zehan Wang&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Batch Normalization in Neural Networks</title>
      <link>http://navinpai.github.io/ga/post/batch-normalization/</link>
      <pubDate>Fri, 07 Oct 2016 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>http://navinpai.github.io/ga/post/batch-normalization/</guid>
      <description>&lt;p&gt;Read a couple of interesting papers today from 2015 about using Batch Normalization in neural networks. Batch Normalization basically means that we normalize each activation individually. The reason for this, quoting the &lt;a href=&#34;https://arxiv.org/abs/1502.03167v3&#34;&gt;original paper&lt;/a&gt;&lt;/p&gt;











  





  


&lt;blockquote&gt;
  &lt;p&gt;
Training Deep Neural Networks is complicated by the fact that the distribution of each layer&#39;s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift.
&lt;/p&gt;
    &lt;strong&gt;&lt;/strong&gt;
    
      
    
&lt;/blockquote&gt;

&lt;p&gt;Their paper is a fascinting deep dive into the math of how layers are affected by the input, and how this covariate shift can be reduced by applying batch normalizations. Using batch normalization means we can use higher learning rates (since gradients do not explode or vanish), making the network more resilient. In fact, in their results they showed how they could use the batch normalized network to achieve the same accuracy as the vanilla network with half the training steps, and with higher learning rates upto 14x fewer steps!&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;./images/batch-norm.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;The Batch Normalization Transform&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;This paper set up a bunch of other interesting papers which explored the same concept of batch normalization, and I&amp;rsquo;ll talk about two of them below.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/abs/1510.01378&#34;&gt;first one&lt;/a&gt; notes that the original paper applies only to feed forward neural networks and attempts to explore batch normalization in Recurrent Neural Networks. They apply batch normalization on the input-to-hidden transition and oberved faster training and greater overfitting as well. The fact that the generalization performance didn&amp;rsquo;t seem to improve with batch normalization was opposite to what was seen for feedforward neural nets.&lt;/p&gt;

&lt;p&gt;However, a couple of months ago, &lt;a href=&#34;https://arxiv.org/abs/1603.09025v4&#34;&gt;another paper&lt;/a&gt; visited the same idea albeit in a slightly different manner &lt;em&gt;(in fact one of the authors is on both the papers)&lt;/em&gt;. Instead of applying the normalization on the input-to-hidden transition, they apply it horizontally between timesteps, using the consideration that RNNs are deepest in the time direction. Lo and behold, they now have the state of the art results (greater generalizability) with lesser convergence time.&lt;/p&gt;

&lt;p&gt;Truly an excellent set of papers exploring what seems, at first read, as a minor modification to an algorithm. Go on, read the papers!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1502.03167v3&#34;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; &lt;em&gt;Sergey Ioffe, Christian Szegedy&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1510.01378&#34;&gt;Batch Normalized Recurrent Neural Networks&lt;/a&gt; &lt;em&gt;César Laurent, Gabriel Pereyra, Philémon Brakel, Ying Zhang, Yoshua Bengio&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.09025v4&#34;&gt;Recurrent Batch Normalization&lt;/a&gt; &lt;em&gt;Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Convolutional Neural Networks Work</title>
      <link>http://navinpai.github.io/ga/post/how-cnns-work/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>http://navinpai.github.io/ga/post/how-cnns-work/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/FmpDIaiMIeA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;p&gt;Understanding convolutions is the basic foundation to understand how CNNs work. Unfortunately, a lot of literature that exists jumps directly into the math without first trying to explain intuitively what convolutions are, and how they can be used for Deep Learning applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/_brohrer_&#34;&gt;Brandon Rohrer&lt;/a&gt; from Microsoft put out an amazing video (&lt;em&gt;and an associated blog post&lt;/em&gt;) trying to provide an intuitive explanation of the same.&lt;/p&gt;

&lt;p&gt;Some quick notes:










  





  


&lt;blockquote&gt;
  &lt;p&gt;
  When presented with a new image, the CNN doesnWhen presented with a new image, the CNN doesn’t know exactly where these features will match so it tries them everywhere, in every possible position. In calculating the match to a feature across the whole image, we make it a filter. The math we use to do this is called convolution, from which Convolutional Neural Networks take their name.âWhen presented with a new image, the CNN doesn’t know exactly where these features will match so it tries them everywhere, in every possible position. In calculating the match to a feature across the whole image, we make it a filter. The math we use to do this is called convolution, from which Convolutional Neural Networks take their name.t know exactly where these features will match so it tries them everywhere, in every possible position. In calculating the match to a feature across the whole image, we make it a filter. The math we use to do this is called convolution, from which Convolutional Neural Networks take their name.
&lt;/p&gt;
    &lt;strong&gt;&lt;/strong&gt;
    
      
    
&lt;/blockquote&gt;&lt;/p&gt;

&lt;p&gt;









  





  


&lt;blockquote&gt;
  &lt;p&gt;
Pooling is a way to take large images and shrink them down while preserving the most important information in them. [...]. It consists of stepping a small window across an image and taking the maximum value from the window at each step. In practice, a window 2 or 3 pixels on a side and steps of 2 pixels work well.
&lt;/p&gt;
    &lt;strong&gt;&lt;/strong&gt;
    
      
    
&lt;/blockquote&gt;&lt;/p&gt;











  





  


&lt;blockquote&gt;
  &lt;p&gt;
[on Backpropogation:] Each image the CNN processes results in a vote. The amount of wrongness in the vote, the error, tells us how good our features and weights are. The features and weights can then be adjusted to make the error less. Each value is adjusted a little higher and a little lower, and the new error computed each time. Whichever adjustment makes the error less is kept. After doing this for every feature pixel in every convolutional layer and every weight in every fully connected layer, the new weights give an answer that works slightly better for that image.
&lt;/p&gt;
    &lt;strong&gt;&lt;/strong&gt;
    
      
    
&lt;/blockquote&gt;

&lt;p&gt;And finally, something that really shows how we&amp;rsquo;re just at the beginning of the deep learning revolution, especially when have hundreds of hidden layers in the network:&lt;/p&gt;











  





  


&lt;blockquote&gt;
  &lt;p&gt;
With so many combinations and permutations, only a small fraction of the possible CNN configurations have been tested. CNN designs tend to be driven by accumulated community knowledge, with occasional deviations showing surprising jumps in performance.
&lt;/p&gt;
    &lt;strong&gt;&lt;/strong&gt;
    
      
    
&lt;/blockquote&gt;

&lt;p&gt;I&amp;rsquo;d highly recommend you take the time out to read the blog post and/or watch the accompanying video.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://brohrer.github.io/how_convolutional_neural_networks_work.html&#34;&gt;Brandon Rohrer&amp;rsquo;s blog&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to Gradient Ascent</title>
      <link>http://navinpai.github.io/ga/post/welcome/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 UTC</pubDate>
      <author>Alexander Ivanov</author>
      <guid>http://navinpai.github.io/ga/post/welcome/</guid>
      <description>&lt;p&gt;Welcome to Gradient Ascent. This is a Machine Learning/Deep Learning blog written by me, &lt;a href=&#34;http://lifeofnav.in&#34;&gt;Navin Pai&lt;/a&gt;. I graduated in 2015 from the &lt;a href=&#34;http://www.iiitb.ac.in&#34;&gt;International Institute of Information Technology - Bangalore&lt;/a&gt; and am currently working as Data Engineer at &lt;a href=&#34;http://www.bloomreach.com&#34;&gt;Bloomreach Inc.&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been fascinated by the world of ML/DL for the last few years, and like every other Computer Science researcher, I&amp;rsquo;m trying to stay up to date with the latest in this rapidly changing field.&lt;/p&gt;

&lt;p&gt;This blog is simply an attempt to document that journey. The idea is to document  a good mix of both research papers as well as code, to develop an overall understanding of the directions we&amp;rsquo;re moving in, not just as researchers but as industry experts as well.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;./images/choose-your-pill.jpg&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;So what are we waiting for? Choose your pill!&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>
